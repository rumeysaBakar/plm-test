import streamlit as st
import json
from openai import OpenAI
import PyPDF2
import io
import os
import difflib
import re
from collections import Counter
from dotenv import load_dotenv
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

load_dotenv()

st.set_page_config(
    page_title="AI Twin Demo",
    page_icon="",
    layout="wide"
)

if "step" not in st.session_state:
    st.session_state.step = 1
if "profile_data" not in st.session_state:
    st.session_state.profile_data = {}
if "uploaded_content" not in st.session_state:
    st.session_state.uploaded_content = []
if "plm_profile" not in st.session_state:
    st.session_state.plm_profile = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "conversation_memory" not in st.session_state:
    st.session_state.conversation_memory = []



def analyze_text_metrics(text: str) -> dict:
    """Metin metriklerini hesaplama"""
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    words = text.split()

    # Ortalama c√ºmle uzunluƒüu
    avg_sentence_length = len(words) / len(sentences) if sentences else 0

    # Kelime √ße≈üitliliƒüi
    word_diversity = len(set(words)) / len(words) if words else 0

    formal_words = ['ancak', 'bununla birlikte', 'dolayƒ±sƒ±yla', 'nitekim', '√∂te yandan',
                    'sonu√ß olarak', 'bu baƒülamda', '≈ü√∂yle ki', 'binaenaleyh']
    casual_words = ['yani', 'aslƒ±nda', 'hani', 'i≈üte', 'mesela', 'falan', '≈üey', 'ama']

    text_lower = text.lower()
    formal_count = sum(1 for w in formal_words if w in text_lower)
    casual_count = sum(1 for w in casual_words if w in text_lower)

    hedging_words = ['belki', 'muhtemelen', 'sanƒ±rƒ±m', 'galiba', 'olabilir', 'd√º≈ü√ºn√ºyorum',
                     'tahminimce', 'gibi g√∂r√ºn√ºyor', 'bir bakƒ±ma']
    definitive_words = ['kesinlikle', 'mutlaka', '≈ü√ºphesiz', 'elbette', 'tabii ki',
                        'ku≈ükusuz', 'a√ßƒ±k√ßa', 'net olarak']

    hedging_count = sum(1 for w in hedging_words if w in text_lower)
    definitive_count = sum(1 for w in definitive_words if w in text_lower)

    emoji_count = len(re.findall(r'[^\w\s,.\-:;\'\"()\[\]{}!?]', text))
    exclamation_count = text.count('!')

    return {
        'total_words': len(words),
        'total_sentences': len(sentences),
        'avg_sentence_length': round(avg_sentence_length, 1),
        'word_diversity': round(word_diversity * 100, 1),
        'formal_markers': formal_count,
        'casual_markers': casual_count,
        'hedging_markers': hedging_count,
        'definitive_markers': definitive_count,
        'emoji_count': emoji_count,
        'exclamation_count': exclamation_count
    }


def get_word_diff(text1: str, text2: str) -> list:
    """ƒ∞ki metin arasƒ±ndaki kelime bazlƒ± farklar"""
    words1 = text1.split()
    words2 = text2.split()

    matcher = difflib.SequenceMatcher(None, words1, words2)

    diff_result = []
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'replace':
            diff_result.append({
                'type': 'replace',
                'old': ' '.join(words1[i1:i2]),
                'new': ' '.join(words2[j1:j2])
            })
        elif tag == 'delete':
            diff_result.append({
                'type': 'delete',
                'old': ' '.join(words1[i1:i2]),
                'new': ''
            })
        elif tag == 'insert':
            diff_result.append({
                'type': 'insert',
                'old': '',
                'new': ' '.join(words2[j1:j2])
            })

    return diff_result


def analyze_plm_changes(raw: str, plm: str, client: OpenAI) -> dict:

    analysis_prompt = f"""
ƒ∞ki metin arasƒ±ndaki stil farklarƒ±nƒ± analiz et:

HAM METƒ∞N:
{raw}

PLM METƒ∞N:
{plm}

A≈üaƒüƒ±daki kategorilerde deƒüi≈üiklikleri tespit et ve JSON formatƒ±nda yanƒ±tla:

{{
    "tone_changes": [
        {{"original": "orijinal ifade", "changed_to": "deƒüi≈ütirilmi≈ü ifade", "reason": "neden deƒüi≈ütirildi"}}
    ],
    "formality_changes": [
        {{"original": "orijinal", "changed_to": "yeni", "direction": "more_formal/less_formal"}}
    ],
    "sentence_structure_changes": [
        {{"description": "ne deƒüi≈üti", "example": "√∂rnek"}}
    ],
    "added_phrases": ["eklenen karakteristik ifadeler"],
    "removed_phrases": ["√ßƒ±karƒ±lan ifadeler"],
    "certainty_changes": [
        {{"original": "orijinal", "changed_to": "yeni", "direction": "more_certain/less_certain"}}
    ],
    "overall_summary": "Genel olarak PLM metni nasƒ±l farklƒ±la≈ütƒ±rdƒ± - 2-3 c√ºmle"
}}
"""

    response = client.chat.completions.create(
        model="gpt-5.1-chat-latest",
        messages=[{"role": "user", "content": analysis_prompt}],
        response_format={"type": "json_object"}
    )

    return json.loads(response.choices[0].message.content)


def render_diff_analysis(raw: str, plm: str, ai_analysis: dict = None):
    """Diff analizini g√∂rselle≈ütir"""

    st.markdown("###  PLM D√∂n√º≈ü√ºm Analizi")

    # Metrik kar≈üƒ±la≈ütƒ±rmasƒ±
    raw_metrics = analyze_text_metrics(raw)
    plm_metrics = analyze_text_metrics(plm)

    # Metrik kartlarƒ±
    st.markdown("####  Sayƒ±sal Kar≈üƒ±la≈ütƒ±rma")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        delta = plm_metrics['avg_sentence_length'] - raw_metrics['avg_sentence_length']
        st.metric(
            "Ort. C√ºmle Uzunluƒüu",
            f"{plm_metrics['avg_sentence_length']} kelime",
            delta=f"{delta:+.1f}",
            delta_color="off"
        )

    with col2:
        delta = plm_metrics['formal_markers'] - raw_metrics['formal_markers']
        st.metric(
            "Formal ƒ∞fadeler",
            plm_metrics['formal_markers'],
            delta=f"{delta:+d}",
            delta_color="off"
        )

    with col3:
        delta = plm_metrics['casual_markers'] - raw_metrics['casual_markers']
        st.metric(
            "G√ºnl√ºk ƒ∞fadeler",
            plm_metrics['casual_markers'],
            delta=f"{delta:+d}",
            delta_color="off"
        )

    with col4:
        delta = plm_metrics['hedging_markers'] - raw_metrics['hedging_markers']
        st.metric(
            "Belirsizlik ƒ∞fadeleri",
            plm_metrics['hedging_markers'],
            delta=f"{delta:+d}",
            delta_color="off"
        )

    # Detaylƒ± metrik tablosu
    with st.expander(" T√ºm Metrikler", expanded=False):
        metrics_data = {
            "Metrik": [
                "Toplam Kelime",
                "Toplam C√ºmle",
                "Ort. C√ºmle Uzunluƒüu",
                "Kelime √áe≈üitliliƒüi (%)",
                "Formal ƒ∞fadeler",
                "G√ºnl√ºk ƒ∞fadeler",
                "Belirsizlik ƒ∞fadeleri",
                "Kesinlik ƒ∞fadeleri",
                "√únlem Sayƒ±sƒ±"
            ],
            "Ham √áƒ±ktƒ±": [
                raw_metrics['total_words'],
                raw_metrics['total_sentences'],
                raw_metrics['avg_sentence_length'],
                raw_metrics['word_diversity'],
                raw_metrics['formal_markers'],
                raw_metrics['casual_markers'],
                raw_metrics['hedging_markers'],
                raw_metrics['definitive_markers'],
                raw_metrics['exclamation_count']
            ],
            "PLM √áƒ±ktƒ±": [
                plm_metrics['total_words'],
                plm_metrics['total_sentences'],
                plm_metrics['avg_sentence_length'],
                plm_metrics['word_diversity'],
                plm_metrics['formal_markers'],
                plm_metrics['casual_markers'],
                plm_metrics['hedging_markers'],
                plm_metrics['definitive_markers'],
                plm_metrics['exclamation_count']
            ],
            "Fark": [
                plm_metrics['total_words'] - raw_metrics['total_words'],
                plm_metrics['total_sentences'] - raw_metrics['total_sentences'],
                round(plm_metrics['avg_sentence_length'] - raw_metrics['avg_sentence_length'], 1),
                round(plm_metrics['word_diversity'] - raw_metrics['word_diversity'], 1),
                plm_metrics['formal_markers'] - raw_metrics['formal_markers'],
                plm_metrics['casual_markers'] - raw_metrics['casual_markers'],
                plm_metrics['hedging_markers'] - raw_metrics['hedging_markers'],
                plm_metrics['definitive_markers'] - raw_metrics['definitive_markers'],
                plm_metrics['exclamation_count'] - raw_metrics['exclamation_count']
            ]
        }
        st.dataframe(metrics_data, use_container_width=True)

    # Kelime bazlƒ± diff
    st.markdown("####  Kelime Bazlƒ± Deƒüi≈üiklikler")

    word_diffs = get_word_diff(raw, plm)

    if word_diffs:
        changes_found = False
        for diff in word_diffs[:10]:
            if diff['type'] == 'replace':
                changes_found = True
                col1, col2, col3 = st.columns([2, 1, 2])
                with col1:
                    st.markdown(f"~~{diff['old'][:50]}...~~" if len(diff['old']) > 50 else f"~~{diff['old']}~~")
                with col2:
                    st.markdown("‚û°Ô∏è")
                with col3:
                    st.markdown(f"**{diff['new'][:50]}...**" if len(diff['new']) > 50 else f"**{diff['new']}**")

        if not changes_found:
            st.info("Kelime bazlƒ± b√ºy√ºk deƒüi≈üiklik tespit edilmedi.")

    # AI Analizi
    if ai_analysis:
        st.markdown("####  AI Deƒüi≈üiklik Analizi")

        # Genel √∂zet
        st.info(f"**√ñzet:** {ai_analysis.get('overall_summary', 'Analiz yapƒ±lamadƒ±')}")

        # Ton deƒüi≈üiklikleri
        tone_changes = ai_analysis.get('tone_changes', [])
        if tone_changes:
            with st.expander(f" Ton Deƒüi≈üiklikleri ({len(tone_changes)})", expanded=True):
                for change in tone_changes[:5]:
                    st.markdown(f"""
                    - **Orijinal:** {change.get('original', '')}
                    - **Yeni:** {change.get('changed_to', '')}
                    - **Sebep:** _{change.get('reason', '')}_
                    """)
                    st.divider()

        # Formalite deƒüi≈üiklikleri
        formality_changes = ai_analysis.get('formality_changes', [])
        if formality_changes:
            with st.expander(f" Formalite Deƒüi≈üiklikleri ({len(formality_changes)})"):
                for change in formality_changes[:5]:
                    direction = " Daha Formal" if change.get('direction') == 'more_formal' else " Daha G√ºnl√ºk"
                    st.markdown(f"- {direction}: _{change.get('original', '')}_ ‚Üí **{change.get('changed_to', '')}**")

        # Kesinlik deƒüi≈üiklikleri
        certainty_changes = ai_analysis.get('certainty_changes', [])
        if certainty_changes:
            with st.expander(f" Kesinlik Deƒüi≈üiklikleri ({len(certainty_changes)})"):
                for change in certainty_changes[:5]:
                    direction = "Daha Kesin" if change.get('direction') == 'more_certain' else "Ô∏è Daha Belirsiz"
                    st.markdown(f"- {direction}: _{change.get('original', '')}_ ‚Üí **{change.get('changed_to', '')}**")

        # Eklenen ifadeler
        added = ai_analysis.get('added_phrases', [])
        if added:
            with st.expander(f"‚ûï Eklenen ƒ∞fadeler ({len(added)})"):
                for phrase in added:
                    st.markdown(f"- **{phrase}**")

        # √áƒ±karƒ±lan ifadeler
        removed = ai_analysis.get('removed_phrases', [])
        if removed:
            with st.expander(f"‚ûñ √áƒ±karƒ±lan ƒ∞fadeler ({len(removed)})"):
                for phrase in removed:
                    st.markdown(f"- ~~{phrase}~~")



def extract_style_examples(uploaded_content: list, client: OpenAI) -> list:

    if not uploaded_content:
        return []

    sample_text = " ".join(uploaded_content)[:3000]

    extraction_prompt = """
A≈üaƒüƒ±daki metinden, yazarƒ±n karakteristik 3-5 c√ºmle √∂rneƒüi √ßƒ±kar.
Bu c√ºmleler yazarƒ±n tipik c√ºmle yapƒ±sƒ±nƒ±, kullandƒ±ƒüƒ± baƒüla√ßlarƒ± ve ifade tarzƒ±nƒ± yansƒ±tmalƒ±.

Metin:
""" + sample_text + """

Sadece c√ºmleleri liste halinde ver, ba≈üka a√ßƒ±klama yapma:
1. [c√ºmle]
2. [c√ºmle]
3. [c√ºmle]
"""

    try:
        response = client.chat.completions.create(
            model="gpt-5.1-chat-latest",
            messages=[{"role": "user", "content": extraction_prompt}]

        )

        content = response.choices[0].message.content
        examples = []
        for line in content.split('\n'):
            line = line.strip()
            if line and (line[0].isdigit() or line.startswith('-')):
                cleaned = re.sub(r'^[\d\-\.\)\s]+', '', line).strip()
                if cleaned and len(cleaned) > 10:
                    examples.append(cleaned)

        return examples[:5]  # Max 5 √∂rnek
    except:
        return []


def calculate_similarity(text1: str, text2: str) -> float:
    try:
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
        return float(similarity[0][0])
    except:
        return 0.0


def is_in_expertise_area(question: str, expertise_areas: list) -> bool:
    if not expertise_areas:
        return False

    question_lower = question.lower()
    expertise_text = " ".join(expertise_areas).lower()

    question_words = set(question_lower.split())
    expertise_words = set(expertise_text.split())

    common_words = question_words.intersection(expertise_words)
    if len(common_words) > 0:
        return True

    # Veya kosin√ºs benzerliƒüi
    similarity = calculate_similarity(question_lower, expertise_text)
    return similarity > 0.1


def get_conversation_context() -> str:
    recent_messages = st.session_state.conversation_memory[-3:]
    if not recent_messages:
        return ""

    context_parts = []
    for msg in recent_messages:
        role = "Kullanƒ±cƒ±" if msg["role"] == "user" else "Asistan"
        context_parts.append(f"{role}: {msg['content'][:200]}...")

    return "\n".join(context_parts)



def extract_pdf_text(pdf_file) -> str:
    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file.read()))
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        return f"[PDF okuma hatasƒ±: {e}]"


def build_plm_profile(profile_data: dict, uploaded_content: list, client: OpenAI) -> dict:
    all_content = f"""
KULLANICI PROFƒ∞L Bƒ∞LGƒ∞LERƒ∞:
- Ad Soyad: {profile_data.get('name', 'Belirtilmedi')}
- Ya≈ü: {profile_data.get('age', 'Belirtilmedi')}
- Meslek: {profile_data.get('profession', 'Belirtilmedi')}
- Deneyim: {profile_data.get('experience_years', 'Belirtilmedi')} yƒ±l
- Yaptƒ±ƒüƒ± √áalƒ±≈ümalar: {profile_data.get('works', 'Belirtilmedi')}
- En B√ºy√ºk Ba≈üarƒ±sƒ±: {profile_data.get('achievement', 'Belirtilmedi')}
- Uzmanlƒ±k Alanlarƒ±: {profile_data.get('expertise', 'Belirtilmedi')}
- Neden Uzman: {profile_data.get('why_expert', 'Belirtilmedi')}

Y√úKLENEN D√ñK√úMANLARDAN ƒ∞√áERƒ∞K:
{chr(10).join(uploaded_content) if uploaded_content else 'D√∂k√ºman y√ºklenmedi'}
"""

    extraction_prompt = """
Sen bir PLM (Personal Language Model) profil analisti olarak √ßalƒ±≈üƒ±yorsun. 
Verilen kullanƒ±cƒ± bilgilerinden 3 katmanlƒ± bir ki≈üilik profili √ßƒ±karmalƒ±sƒ±n.

√áIKARMANI GEREKEN 3 KATMAN:

1. **KNOWLEDGE_LAYER (Bilgi Katmanƒ±)**
   - Ki≈üinin uzmanlƒ±k alanlarƒ±
   - Bildiƒüi konular ve derinlik seviyeleri
   - Sekt√∂rel bilgisi
   - Teknik yetkinlikleri

2. **REASONING_LAYER (Muhakeme Katmanƒ±)**
   - Problem √ß√∂zme yakla≈üƒ±mƒ±
   - Karar verme tarzƒ± (temkinli mi, hƒ±zlƒ± mƒ±?)
   - Hangi konularda kesin konu≈üur, hangilerinde belirsiz kalƒ±r?
   - Analitik mi yoksa sezgisel mi?
   - Risk toleransƒ±

3. **LANGUAGE_LAYER (Dil/Ton Katmanƒ±)**
   - C√ºmle uzunluƒüu tercihi (kƒ±sa/orta/uzun)
   - Formalite seviyesi (resmi/yarƒ±-resmi/samimi)
   - Kesinlik derecesi (kesin ifadeler mi, yumu≈üatƒ±lmƒ±≈ü mƒ±?)
   - Karakteristik kelimeler veya kalƒ±plar
   - A√ßƒ±klama tarzƒ± (√∂rneklerle mi, teorik mi, pratik mi?)
   - Emoji/√ºnlem kullanƒ±mƒ±

A≈üaƒüƒ±daki JSON formatƒ±nda yanƒ±t ver (ba≈üka hi√ßbir ≈üey yazma):

{
    "knowledge_layer": {
        "primary_expertise": ["alan1", "alan2"],
        "secondary_knowledge": ["alan1", "alan2"],
        "depth_level": "beginner/intermediate/expert/master",
        "industry_context": "sekt√∂r bilgisi"
    },
    "reasoning_layer": {
        "decision_style": "analytical/intuitive/balanced",
        "confidence_areas": ["kesin konu≈ütuƒüu alanlar"],
        "uncertain_areas": ["belirsiz kaldƒ±ƒüƒ± alanlar"],
        "problem_approach": "systematic/creative/pragmatic",
        "risk_tolerance": "low/medium/high"
    },
    "language_layer": {
        "sentence_length": "short/medium/long",
        "formality": "formal/semi-formal/casual",
        "certainty_level": "definitive/hedged/mixed",
        "explanation_style": "examples/theoretical/practical",
        "characteristic_phrases": ["√∂rnek kalƒ±p1", "√∂rnek kalƒ±p2"],
        "tone": "professional/friendly/authoritative/humble"
    },
    "persona_summary": "Bu ki≈üinin tek c√ºmlelik √∂zeti"
}
"""

    response = client.chat.completions.create(
        model="gpt-5.1-chat-latest",
        messages=[
            {"role": "system", "content": extraction_prompt},
            {"role": "user", "content": all_content}
        ],

        response_format={"type": "json_object"}
    )

    profile = json.loads(response.choices[0].message.content)

    style_examples = extract_style_examples(uploaded_content, client)
    if style_examples:
        profile["language_layer"]["style_examples"] = style_examples

    return profile


def generate_raw_response(question: str, plm_profile: dict, client: OpenAI) -> str:
    knowledge = plm_profile.get("knowledge_layer", {})
    reasoning = plm_profile.get("reasoning_layer", {})

    expertise_areas = knowledge.get('primary_expertise', []) + knowledge.get('secondary_knowledge', [])
    in_expertise = is_in_expertise_area(question, expertise_areas)

    dynamic_certainty = "definitive" if in_expertise else "hedged"

    conversation_context = get_conversation_context()
    memory_context = ""
    if conversation_context:
        memory_context = f"\n√ñNCEKƒ∞ KONU≈ûMA BAƒûLAMI:\n{conversation_context}\n"

    system_prompt = f"""
Sen bir uzman asistanƒ±sƒ±n. A≈üaƒüƒ±daki bilgi ve muhakeme profiline g√∂re soruyu yanƒ±tla.

Bƒ∞LGƒ∞ PROFƒ∞Lƒ∞:
- Ana Uzmanlƒ±k: {knowledge.get('primary_expertise', [])}
- ƒ∞kincil Bilgi: {knowledge.get('secondary_knowledge', [])}
- Derinlik: {knowledge.get('depth_level', 'intermediate')}
- Sekt√∂r: {knowledge.get('industry_context', '')}

MUHAKEME PROFƒ∞Lƒ∞:
- Karar Tarzƒ±: {reasoning.get('decision_style', 'balanced')}
- Kesin Olduƒüu Alanlar: {reasoning.get('confidence_areas', [])}
- Belirsiz Alanlar: {reasoning.get('uncertain_areas', [])}
- Problem Yakla≈üƒ±mƒ±: {reasoning.get('problem_approach', 'pragmatic')}
- Dinamik Kesinlik Seviyesi: {dynamic_certainty} (Soru uzmanlƒ±k alanƒ±nda: {in_expertise})
{memory_context}

Soruyu bu profile uygun ≈üekilde, i√ßerik olarak doƒüru ve kapsamlƒ± yanƒ±tla.
Muhakeme profiline g√∂re kesin veya belirsiz ifadeler kullan.
"""

    response = client.chat.completions.create(
        model="gpt-5.1-chat-latest",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ]

    )

    return response.choices[0].message.content


def apply_plm_rewrite(raw_response: str, plm_profile: dict, client: OpenAI) -> str:
    language = plm_profile.get("language_layer", {})
    persona = plm_profile.get("persona_summary", "")

    style_examples = language.get('style_examples', [])
    examples_text = ""
    if style_examples:
        examples_text = "\nKULLANICININ KARAKTERƒ∞STƒ∞K C√úMLE √ñRNEKLERƒ∞ (bu tarzƒ± taklit et):\n" + \
                        "\n".join([f"- {ex}" for ex in style_examples[:3]])

    rewrite_prompt = f"""
Sen bir PLM (Personal Language Model) yeniden yazƒ±m motorusun.

G√ñREV: A≈üaƒüƒ±daki ham cevabƒ±, belirtilen dil profiline g√∂re yeniden yaz.
ƒ∞√ßeriƒüi DEƒûI≈ûTIRME, sadece NASIL s√∂ylendiƒüini deƒüi≈ütir.

Dƒ∞L PROFƒ∞Lƒ∞:
- C√ºmle Uzunluƒüu: {language.get('sentence_length', 'medium')}
- Formalite: {language.get('formality', 'semi-formal')}
- Kesinlik Seviyesi: {language.get('certainty_level', 'mixed')}
- A√ßƒ±klama Tarzƒ±: {language.get('explanation_style', 'practical')}
- Karakteristik Kalƒ±plar: {language.get('characteristic_phrases', [])}
- Ton: {language.get('tone', 'professional')}
{examples_text}

PERSONA: {persona}

KURALLAR:
1. ƒ∞√ßeriƒüin anlamƒ±nƒ± koru
2. C√ºmle yapƒ±sƒ±nƒ± profile g√∂re ayarla
3. Karakteristik kalƒ±plarƒ± doƒüal ≈üekilde ekle
4. Yukarƒ±daki c√ºmle √∂rneklerindeki yapƒ±yƒ± taklit et
5. Tonu tutarlƒ± tut
6. Ger√ßek bir insan yazmƒ±≈ü gibi g√∂r√ºnmeli

YASAKLAR:
- "Buna g√∂re", "Sonu√ß olarak" gibi generic baƒüla√ßlar kullanma (eƒüer kullanƒ±cƒ± kullanmƒ±yorsa)
- Profildeki formalite seviyesi dƒ±≈üƒ±nda bir ton kullanma

HAM CEVAP:
{raw_response}

YENƒ∞DEN YAZILMI≈û CEVAP:
"""

    response = client.chat.completions.create(
        model="gpt-5.1-chat-latest",
        messages=[
            {"role": "user", "content": rewrite_prompt}
        ]
    )

    return response.choices[0].message.content


def generate_twin_response(question: str, plm_profile: dict, client: OpenAI) -> tuple[str, str, dict]:

    raw_response = generate_raw_response(question, plm_profile, client)
    plm_response = apply_plm_rewrite(raw_response, plm_profile, client)

    # AI analizi yap
    try:
        ai_analysis = analyze_plm_changes(raw_response, plm_response, client)
    except:
        ai_analysis = None

    return raw_response, plm_response, ai_analysis



def render_step_indicator():
    """Adƒ±m g√∂stergesi"""
    cols = st.columns(3)
    steps = ["Profil Bilgileri", " D√∂k√ºman Y√ºkleme", " AI Twin"]

    for i, (col, step) in enumerate(zip(cols, steps), 1):
        with col:
            if i == st.session_state.step:
                st.markdown(f" {step}")
            elif i < st.session_state.step:
                st.markdown(f" {step}")
            else:
                st.markdown(f" {step}")

    st.divider()


def render_step1():
    """Adƒ±m 1: Profil Sorularƒ±"""
    st.header(" Kendinizi Tanƒ±tƒ±n")
    st.caption("AI ikizinizi olu≈üturmak i√ßin size birka√ß soru soracaƒüƒ±z.")

    with st.form("profile_form"):
        col1, col2 = st.columns(2)

        with col1:
            name = st.text_input("Adƒ±nƒ±z Soyadƒ±nƒ±z *", value=st.session_state.profile_data.get("name", ""))
            age = st.number_input("Ya≈üƒ±nƒ±z", min_value=18, max_value=100,
                                  value=st.session_state.profile_data.get("age", 30))
            profession = st.text_input("Mesleƒüiniz *", value=st.session_state.profile_data.get("profession", ""))
            experience_years = st.number_input("Bu meslekte ka√ß yƒ±ldƒ±r √ßalƒ±≈üƒ±yorsunuz?", min_value=0, max_value=60,
                                               value=st.session_state.profile_data.get("experience_years", 5))

        with col2:
            works = st.text_area("Bu zamana kadar yaptƒ±ƒüƒ±nƒ±z √∂nemli √ßalƒ±≈ümalar neler?",
                                 value=st.session_state.profile_data.get("works", ""), height=100)
            achievement = st.text_area("Mesleƒüinizdeki en b√ºy√ºk ba≈üarƒ±nƒ±z ne?",
                                       value=st.session_state.profile_data.get("achievement", ""), height=100)

        expertise = st.text_area("Uzman olduƒüunuz konular neler?",
                                 value=st.session_state.profile_data.get("expertise", ""), height=80)
        why_expert = st.text_area("Bu konularda neden uzman olduƒüunuzu d√º≈ü√ºn√ºyorsunuz?",
                                  value=st.session_state.profile_data.get("why_expert", ""), height=80)

        submitted = st.form_submit_button("ƒ∞leri ‚Üí", use_container_width=True, type="primary")

        if submitted:
            if not name or not profession:
                st.error("L√ºtfen zorunlu alanlarƒ± doldurun!")
            else:
                st.session_state.profile_data = {
                    "name": name,
                    "age": age,
                    "profession": profession,
                    "experience_years": experience_years,
                    "works": works,
                    "achievement": achievement,
                    "expertise": expertise,
                    "why_expert": why_expert
                }
                st.session_state.step = 2
                st.rerun()


def render_step2():
    st.header(" D√∂k√ºmanlarƒ±nƒ±zƒ± Y√ºkleyin")
    st.caption(
        "Mesleƒüinizle ve kendinizle alakalƒ± d√∂k√ºmanlar y√ºkleyin. Bu d√∂k√ºmanlar AI ikizinizin bilgi tabanƒ±nƒ± olu≈üturacak.")

    with st.expander("üìã Profil √ñzetiniz", expanded=False):
        st.json(st.session_state.profile_data)

    uploaded_files = st.file_uploader(
        "PDF, TXT veya metin dosyalarƒ± y√ºkleyin",
        type=["pdf", "txt", "md"],
        accept_multiple_files=True,
        help="≈ûu an demo i√ßin sadece metin tabanlƒ± dosyalar destekleniyor."
    )

    if uploaded_files:
        st.success(f"{len(uploaded_files)} dosya se√ßildi")

        content_list = []
        for file in uploaded_files:
            st.write(f" {file.name}")

            if file.type == "application/pdf":
                text = extract_pdf_text(file)
                content_list.append(f"[{file.name}]:\n{text[:2000]}...")
            else:
                text = file.read().decode("utf-8", errors="ignore")
                content_list.append(f"[{file.name}]:\n{text[:2000]}...")

        st.session_state.uploaded_content = content_list

    st.divider()
    st.subheader(" Manuel Metin")
    manual_text = st.text_area(
        "Kendinizi anlatan, yazƒ± stilinizi g√∂steren √∂rnek metinler ekleyin",
        height=150,
        placeholder="√ñrneƒüin: Daha √∂nce yazdƒ±ƒüƒ±nƒ±z makaleler, blog yazƒ±larƒ±, e-postalar..."
    )

    if manual_text:
        if manual_text not in st.session_state.uploaded_content:
            st.session_state.uploaded_content.append(f"[Manuel Giri≈ü]:\n{manual_text}")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("‚Üê Geri", use_container_width=True):
            st.session_state.step = 1
            st.rerun()
    with col2:
        if st.button("AI Twin Olu≈ütur ‚Üí", use_container_width=True, type="primary"):
            st.session_state.step = 3
            st.rerun()


def render_step3():
    st.header(" AI Twin'iniz Hazƒ±r")

    api_key = os.getenv("OPENAI_API_KEY")

    if not api_key:
        st.error(" OPENAI_API_KEY ortam deƒüi≈ükeni bulunamadƒ±.")
        st.info("L√ºtfen .env dosyasƒ±nƒ± kontrol edin.")
        return

    client = OpenAI(api_key=api_key)

    if st.session_state.plm_profile is None:
        with st.spinner(" PLM Profili olu≈üturuluyor..."):
            try:
                st.session_state.plm_profile = build_plm_profile(
                    st.session_state.profile_data,
                    st.session_state.uploaded_content,
                    client
                )
                st.success(" PLM Profili olu≈üturuldu!")

                style_examples = st.session_state.plm_profile.get("language_layer", {}).get("style_examples", [])
                if style_examples:
                    with st.expander(" Tespit Edilen Stil √ñrnekleri", expanded=True):
                        for i, ex in enumerate(style_examples, 1):
                            st.markdown(f"{i}. {ex}")

            except Exception as e:
                st.error(f"Hata: {e}")
                return

    with st.expander(" PLM Profil Detaylarƒ±", expanded=False):
        col1, col2, col3 = st.columns(3)

        with col1:
            st.subheader("Bilgi Katmanƒ±")
            st.json(st.session_state.plm_profile.get("knowledge_layer", {}))

        with col2:
            st.subheader(" Muhakeme Katmanƒ±")
            st.json(st.session_state.plm_profile.get("reasoning_layer", {}))

        with col3:
            st.subheader(" Dil/Ton Katmanƒ±")
            lang_layer = st.session_state.plm_profile.get("language_layer", {}).copy()
            if "style_examples" in lang_layer:
                lang_layer["style_examples"] = f"[{len(lang_layer['style_examples'])} √∂rnek]"
            st.json(lang_layer)

        st.info(f"**Persona:** {st.session_state.plm_profile.get('persona_summary', '')}")

    st.divider()

    st.subheader(f"{st.session_state.profile_data.get('name', 'AI Twin')} ile Konu≈üun")

    # Mesaj ge√ßmi≈üi
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            if msg["role"] == "assistant" and "raw" in msg:
                tab1, tab2, tab3 = st.tabs([" PLM √áƒ±ktƒ±sƒ±", " Ham √áƒ±ktƒ±", " Fark Analizi"])
                with tab1:
                    st.write(msg["content"])
                with tab2:
                    st.write(msg["raw"])
                with tab3:
                    render_diff_analysis(msg["raw"], msg["content"], msg.get("analysis"))
            else:
                st.write(msg["content"])

    # Yeni mesaj
    if prompt := st.chat_input("AI Twin'inize bir soru sorun..."):
        st.session_state.conversation_memory.append({"role": "user", "content": prompt})

        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        with st.chat_message("assistant"):
            with st.spinner("D√º≈ü√ºn√ºyor..."):
                try:
                    raw_response, plm_response, ai_analysis = generate_twin_response(
                        prompt,
                        st.session_state.plm_profile,
                        client
                    )

                    tab1, tab2, tab3 = st.tabs([" PLM √áƒ±ktƒ±sƒ±", " Ham √áƒ±ktƒ±", " Fark Analizi"])
                    with tab1:
                        st.write(plm_response)
                    with tab2:
                        st.write(raw_response)
                        st.caption("Ô∏è Bu, PLM olmadan √ºretilen ham cevap.")
                    with tab3:
                        render_diff_analysis(raw_response, plm_response, ai_analysis)

                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": plm_response,
                        "raw": raw_response,
                        "analysis": ai_analysis
                    })

                    st.session_state.conversation_memory.append({
                        "role": "assistant",
                        "content": plm_response
                    })

                except Exception as e:
                    st.error(f"Hata: {e}")

    st.sidebar.divider()
    if st.sidebar.button(" Ba≈ütan Ba≈üla", use_container_width=True):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()


def main():
    st.title(" AI Twin Demo")
    st.caption("Personal Language Model (PLM) Katmanƒ± Demonstrasyonu")

    render_step_indicator()

    if st.session_state.step == 1:
        render_step1()
    elif st.session_state.step == 2:
        render_step2()
    elif st.session_state.step == 3:
        render_step3()


if __name__ == "__main__":
    main()
